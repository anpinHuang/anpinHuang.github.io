<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Gredient Boosting</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ben Huang</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About me</a>
</li>
<li>
  <a href="econometrics.html">Econometrics</a>
</li>
<li>
  <a href="dataScienceML.html">Data Science</a>
</li>
<li>
  <a href="Finance.html">Financial Math &amp; Risk</a>
</li>
<li>
  <a href="Programming.html">Programming</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Gredient Boosting</h1>

</div>


<p>In this page we discuss fundementals of building gradient boosting trees for continunous target variable and binary target variable. This is a study note from <a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw">StatQuest with Josh Starmer</a></p>
<hr />
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="gb-for-continuous-variable">GB for Continuous Variable</h3>
</ins>
</div>
<p>Steps for Gradient Boosting for continuous target variable (gradient boost for regression).</p>
<ol style="list-style-type: decimal">
<li>Make initial estimation - all sample average of target variable</li>
<li>Build a regression tree (industry practice is to have ~8 leaf nodes) on pseudo residuals</li>
<li>Make predictions based on the regression tree and the <strong><em>learning rate</em></strong> <span class="math inline">\(\nu\)</span></li>
<li>Calculate the pseudo residuals of the predictions</li>
<li>Build another regression tree based on the pseudo residuals</li>
<li>Make predictions based on the all regression trees above and the learning rate. see prediction formula below.</li>
<li>Repeat 4-6 iteratively until we reach the maximum number of iterations or building a new tree does not reduce the residuals.</li>
</ol>
<p>The idea of the learning is that for each tree that was build, we make a small steps toward (hopefully) to the true value.</p>
<p>When making the first prediction, the predicted value is the sample average of the target variable in the node. It can be shown by the formula below.</p>
<p><strong>Loss Function</strong></p>
<p><span class="math display">\[L(y_i, F(x)) = \sum_{i=1}^{n} \frac{1}{2} (y_i-\hat{y})^2\]</span></p>
<p><strong>Step 1:</strong> initialize the model with constant value <span class="math display">\[F_0(x) = \underset{\gamma}{\operatorname{argmin}} \sum_{i=1}^{n} L(y_i,\gamma)\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> here is the predicted value. By taking the first derivative of the loss function and find <span class="math inline">\(\gamma\)</span> such that first derivative of the loss function equals 0. We get that the <strong><em><span class="math inline">\(\gamma\)</span> is the average of the sample</em></strong>.</p>
<p><strong>Step 2:</strong> for m = 1 to M: (M trees)</p>
<p><strong>2.1 Compute Pseudo Residuals </strong> <span class="math display">\[r_{i,m} = -[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)}\]</span> <span style="color: red;"> <strong><em>note that this derivative <span class="math inline">\([\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]\)</span> is the gradient that the gradient boosting is named after </em></strong> </span></p>
<p><strong>2.2 Fit a regression tree to the Pseudo Residuals <span class="math inline">\(r_{i,m}\)</span> and create terminal regions (leaves) <span class="math inline">\(R_{j,m}\)</span> for <span class="math inline">\(j= 1...J_m\)</span> </strong></p>
<p><strong>2.3 Again, Find the <span class="math inline">\(\gamma\)</span> (output value) that minimize the loss function</strong><br />
<span class="math display">\[\gamma_{j,m} = \underset{\gamma}{\operatorname{argmin}} \sum_{x\in{R_{i,j}}}L(y_i, F_{m-1}(x_i)+\gamma)\]</span> for <span class="math inline">\(j=1...J_m\)</span>. This can be solved analytically. Given our chose of loss function, the <span class="math inline">\(\gamma\)</span> is always the average of the (pseudo) residuals in the same leaf.</p>
<p>where</p>
<ul>
<li><span class="math inline">\(m\)</span> indicates the m-th number of tree and</li>
<li><span class="math inline">\(j\)</span> indicates the j-th number of leave (of the m-th number of tree)</li>
</ul>
<p><strong>2.4 Make new predictions for each sample: </strong> <span class="math display">\[F_m(x) = F_{m-1}(x)+ \nu \sum_{j=1}^{J_m} \gamma_{j,m}I(x\in R_{j,m})\]</span></p>
<p>where <span class="math inline">\(\nu\)</span> is the <strong><em>Learning Rate</em></strong> and is a value between 0 and 1. A small learning rate reduce the effect of each tree on the final production and improve the accurary in the long run.</p>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="gb-for-binary-variable">GB for Binary Variable</h3>
</ins>
</div>
<p>Gradient Boosting for binary target variable is similar to that for continunous target variable, but</p>
<ul>
<li>with different <strong><em>Loss function</em></strong> and</li>
<li><strong><em>The <span class="math inline">\(\gamma\)</span> here is the <span class="math inline">\(log(odds)\)</span>. </em></strong></li>
</ul>
<p><span class="math display">\[-\sum_{i=1}^{n} y_i \cdot log(\hat{p})+(1-y_i) \cdot log(1-\hat{p}) \]</span></p>
<p>Now, we want to make it a function of predicted log-odd ratio, and then simplify it.</p>
<p><span class="math display">\[
\begin{aligned}
-[y_i \cdot log(\hat{p_i})+(1-y_i) \cdot log(1-\hat{p_i})] &amp;=  -y_i \cdot log(\hat{p_i})-(1-y_i) \cdot log(1-\hat{p_i})\\
&amp;= -y_i \cdot log(\hat{p_i})-log(1-\hat{p_i})+y_i \cdot log(1-\hat{p_i}) \\
&amp;= -y_i \cdot [log(\hat{p_i})-log(1-\hat{p_i})]-  log(1-\hat{p_i}) \\
&amp;= -y_i \cdot log(\frac{\hat{p_i}}{1-\hat{p_i}})-  log(1-\hat{p_i}) \\
&amp;= -y_i \cdot log(odds)-  log(1-\hat{p_i}) \\
&amp;= -y_i \cdot log(odds)- log(1-\frac{e^{log(odds)}}{1+e^{log(odds)}}) \\
&amp;= -y_i \cdot log(odds)- log(\frac{1}{1+e^{log(odds)}}) \\
&amp;= -y_i \cdot log(odds)- [log(1)-log({1+e^{log(odds)}})] \\
&amp;= -y_i \cdot log(odds)+log({1+e^{log(odds)}})] \\
\end{aligned}
\]</span></p>
<p>Now take the first derivative of the loss function with respect to <span class="math inline">\(log(odds)\)</span> <span class="math display">\[
\begin{aligned}
\frac{d}{d log(odds)} L &amp;= -y_i+ \frac{e^{log(odds)}}{1+e^{log(odds)}} \\
&amp;= -y_i + \hat{p_i}
\end{aligned}
\]</span></p>
<p>Now we can do the</p>
<p><strong>Step 1:</strong> Initialize the model with constant value <span class="math display">\[F_0(x) = \underset{\gamma}{\operatorname{argmin}} \sum_{i=1}^{n} L(y_i,\gamma)\]</span> where the initial <span class="math inline">\(\hat{p_i}\)</span> for all the samples <span class="math inline">\(i= 1,...,n\)</span> is the sum of <span class="math inline">\(y_i\)</span> divided by the number of observations.</p>
<p><strong>Step 2:</strong> for m = 1 to M: (M trees)</p>
<p><strong>2.1 Compute Pseudo Residuals</strong> <span class="math display">\[r_{i,m} = -[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)}\]</span> for <span class="math inline">\(i = 1,...,n\)</span>. We know from above that <span class="math inline">\(\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)} = -y_i + \hat{p}\)</span>, so <span class="math inline">\(r_{i,m}\)</span> is just the residuals: <span class="math inline">\(y_i- \hat{p}\)</span></p>
<p><strong>2.2 Fit a regression tree to the Pseudo Residuals <span class="math inline">\(r_{i,m}\)</span> and create terminal regions (leaves) <span class="math inline">\(R_{j,m}\)</span> for <span class="math inline">\(j= 1...J_m\)</span></strong></p>
<p><strong>2.3 Again, Find the <span class="math inline">\(\gamma\)</span> (<span class="math inline">\(log(odds)\)</span>) for each termial leaves that minimize the loss function</strong></p>
<p><span class="math display">\[
\begin{aligned}
\gamma_{j,m} &amp;= \underset{\gamma}{\operatorname{argmin}} \sum_{x_i\in{R_{i,j}}}L(y_i, F_{m-1}(x_i)+\gamma) \\
&amp;= \underset{\gamma}{\operatorname{argmin}} \sum_{x_i\in{R_{i,j}}}-y_i \cdot [F_{m-1}(x_i)+ \gamma]+log({1+e^{F_{m-1}(x_i)+\gamma}})]
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(j=1...J_m\)</span>. Solving for this analytically is difficult, therefore we approximate the loss function with second order of Taylor’s expansion</p>
<p><span class="math display">\[
L(y_i, F_{m-1}(x_i)+\gamma) \approx L(y_i, F_{m-1}(x_i) + \frac{d}{d\gamma}L(y_i, F_{m-1}(x_i)\gamma) + \frac{1}{2} \frac{d^2}{d\gamma^2}L(y_i, F_{m-1}(x_i))\gamma^2
\]</span></p>
<p>Now we take the derivative:</p>
<p><span class="math display">\[
\frac{d}{d\gamma}L(y_i, F_{m-1}(x_i)+\gamma) \approx \frac{d}{d\gamma}L(y_i, F_{m-1}(x)) + \frac{d^2}{d\gamma^2}L(y_i, F_{m-1})\gamma
\]</span> Now set it to 0 and solve for <span class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\gamma_{j,m} &amp;= \frac{-1 \cdot \sum_{x_i \in R_{ij}}\frac{d}{d\gamma}L(y_i, F_{m-1}(x_i))}{\sum_{x_i \in R_{ij}}\frac{d^2}{d\gamma^2}L(y_i, F_{m-1})\gamma} \\
&amp;= \frac{\sum_{x_i \in R_{ij}} (y_i-\hat{p_i})}{\sum_{x_i \in R_{ij}}[\hat{p_i} \cdot (1-\hat{p_i})]}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\hat{p_i}\)</span> is the estimated probability from the previous tree.</p>
<p><strong>2.4 Update the prediction:</strong> <span class="math display">\[F_m(x) = F_{m-1}(x) + \nu \sum_{j=1}^{Jm}\gamma I(x \in R_{jm})\]</span></p>
<p><strong>Step 3:</strong> Output <span class="math inline">\(F_{M}(x)\)</span></p>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="xgboost---extreme-gradient-boosting">XGBoost - Extreme Gradient Boosting</h3>
</ins>
</div>
<p>XGBoost stands for extreme gradient boosting. It is different from the original gradient boost by</p>
<ul>
<li>The XGBoost does regression with its <strong><em>unique trees</em></strong></li>
<li><span class="math inline">\(\lambda\)</span> here stands for the <strong><em>regularization parameter</em></strong> (usually default to 0) and</li>
<li><span class="math inline">\(\epsilon\)</span> refers to the <strong><em>learning rate</em></strong> (usually default to 0.3)</li>
<li>The initial prediction starts at 0.5</li>
</ul>
<p><span class="math display">\[\underset{O_{value}}{\operatorname{argmin}}[\sum_{i=1}^{n}L(y_i, F_{m-1}(x)+O_{value})+\gamma T+\frac{1}{2}\lambda O_{value}^2]\]</span> The term <span class="math inline">\(\lambda T\)</span></p>
<p>2nd order Taylor approximation for both regression &amp; classification <span class="math display">\[L(y, )\]</span></p>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
