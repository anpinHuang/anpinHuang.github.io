---
title: "Basic Linear Algebra"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
---

<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Column Space & Rank
</ins>
</div>
* Column space  
  $C(A) = span(\overrightarrow{a_1},\overrightarrow{a_2},\overrightarrow{a_3},\overrightarrow{a_4},\overrightarrow{a_5})$
* Basis for $C(A)$:  
  vectors of span $C(A)$ that are linear independent
  
\[A = 
\begin{bmatrix}
1 & 0 & -1 & 0 & 4 \\
2 & 1 & 0 & 0 & 9 \\
-1 & 2 & 5 & 1& -5\\
1 & -1 & -3 & -2 & 9 \\
\end{bmatrix}
\]



**Reduced row echelon form** by row operation:
\[rref(A) = R = 
\begin{bmatrix}
1 & 0 & -1 & 0 & 4 \\
0 & 1 & 2 & 0 & 1 \\
0 & 0 & 0 & 1& -3\\
0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
\]


The **pivot column** $\overrightarrow{r_1}$, $\overrightarrow{r_2}$, and $\overrightarrow{r_4}$ of matix $R$ are linear independent  
$\Rightarrow$ $\overrightarrow{a_1}$, $\overrightarrow{a_2}$, and $\overrightarrow{a_4}$ are also linear independent  
$\Rightarrow$ $\overrightarrow{a_1}$, $\overrightarrow{a_2}$, and $\overrightarrow{a_4}$ form the basis for $C(A)$  
$\Rightarrow$ $dim(C(A)) = 3$ and $rank(A) = 3$
  
  


<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Positive Definite Matrices
</ins>
</div>

Positive Definite Matrix: Symmetric matrices that has positive eigenvalues

The following 5 tests can be used for determining if the mattrix is symmetric positive definite. All of them are testing the same property but from a different point of view, so fulfilling one test means all others are also fulfilled.

1. All $\lambda_i > 0$
2. Energy $X^TSX > 0$ (all $x \neq 0$) 
3. $S = A^TA$ (independent columns in $A$)
4. All leading determinants > 0
5. All pivots in elimination > 0

The following content shows these 5 tests for matrix $S$

\[ S = 
\begin{bmatrix}
3 & 4\\
4 & 6\\
\end{bmatrix}
\]

1. skip
2. given the matrix is 2 by 2, we can assume \[X=\begin{bmatrix}
x\\
y\\
\end{bmatrix}
\]
\[
\begin{bmatrix}
x & y\\
\end{bmatrix}
\begin{bmatrix}
3 & 4\\
4 & 6\\
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
\end{bmatrix} = 2x^2+6y^2+8xy
\]
    + This can be a loss function $f(x,y)$, and we need $S$ to be positive definite so that $f(x,y)>0$ (convex, or bowl shape). 
    + Gradient descent can be used for finding the minimum. 
    $$\bigtriangledown f = 
    \begin{bmatrix} 
    \frac{\partial f}{\partial x} \\
    \frac{\partial f}{\partial y} \\ 
    \end{bmatrix}$$
    + If the one eigenvalue is very large and the other is very small, then the gradient descent would look like zig-zaging down the mountain instead of going down by straight line.

3. skip
4. 1st leading determinant = 3, 2nd leading determinant = $(3 \cdot 6)-(4 \cdot 4) = 2$
5. The pivots are $3$ and $\frac{2}{3}$ \[ S = 
\begin{bmatrix}
3 & 4\\
4 & 6\\
\end{bmatrix} \sim \begin{bmatrix}
3 & 4\\
0 & \frac{2}{3}\\
\end{bmatrix}
\]


<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Positive Semidefinite Matrices
</ins>
</div>

1. All $\lambda_i \geq 0$
2. Energy $X^TSX \geq 0$ (all $x \neq 0$) 
3. $S = A^TA$ (dependent columns in allowed)
4. All leading determinants $\geq 0$
5. $r$ pivots in elimination > 0, $r \leq n$

example 1: 

\[ S' = 
\begin{bmatrix}
3 & 4\\
4 & \frac{16}{3}\\
\end{bmatrix}
\]

* The determinant $det(S') = 0$ $\Rightarrow$ it has a 0 eigenvalue
* but how do we know the other eigenvalue is 0? By **trace**, Sum of $\lambda$s must be $3+\frac{16}{3} = \frac{25}{3}$. Therefore $\lambda$s must be $0$ and $\frac{25}{3}$

example 2:

\[
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix}
\]

It is a positive semidefinite matrix, with the eigenvalues 3,0,0 because:

* It is a singular matrix
* with rank=1 $\Rightarrow$ there is only 1 non-zero eigenvalue
* The trace is 3 $\Rightarrow$ the only non-zero eigenvalue is 3

Now, we try to decompose the matrix to $A^TA$

\[
\begin{aligned}
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix} &= \lambda_1q_1q_1^T+\lambda_2q_2q_2^T+\lambda_3q_3q_3^T = Q\Lambda Q^T \\
&= 3 \begin{bmatrix}
1/\sqrt{3} \\
1/\sqrt{3} \\
1/\sqrt{3} \\
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}}\\
\end{bmatrix} + 0 + 0
\end{aligned}
\]


Since a covariance matrix is derived by $A^TA$, it is by definition a positive semidefinite a matrix. However, there are circumstances that the "covariance/correlation matrix" is not positive semidefinite when they are not derive by $A^TA$

* **pairwise estimation**: It is common that when there are missing values in the sample data, analyst would estimate the covariance matrix by pairwise estimation, which is easy to do in R.
* **pairwise forecast**: The analyst can also forecast pairwise correlation of more than 2 variables, then in this case the matrix is not positive semidefinite and is not technically a covariance/correlation matrix.



