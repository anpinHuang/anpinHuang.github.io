---
title: "Basic Linear Algebra"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
---

<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Column Space & Rank
</ins>
</div>
* Column space  
  $C(A) = span(\overrightarrow{a_1},\overrightarrow{a_2},\overrightarrow{a_3},\overrightarrow{a_4},\overrightarrow{a_5})$
* Basis for $C(A)$:  
  vectors of span $C(A)$ that are linear independent
  
\[A = 
\begin{bmatrix}
1 & 0 & -1 & 0 & 4 \\
2 & 1 & 0 & 0 & 9 \\
-1 & 2 & 5 & 1& -5\\
1 & -1 & -3 & -2 & 9 \\
\end{bmatrix}
\]



**Reduced row echelon form** by row operation:
\[rref(A) = R = 
\begin{bmatrix}
1 & 0 & -1 & 0 & 4 \\
0 & 1 & 2 & 0 & 1 \\
0 & 0 & 0 & 1& -3\\
0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}
\]


The **pivot column** $\overrightarrow{r_1}$, $\overrightarrow{r_2}$, and $\overrightarrow{r_4}$ of matix $R$ are linear independent  
$\Rightarrow$ $\overrightarrow{a_1}$, $\overrightarrow{a_2}$, and $\overrightarrow{a_4}$ are also linear independent  
$\Rightarrow$ $\overrightarrow{a_1}$, $\overrightarrow{a_2}$, and $\overrightarrow{a_4}$ form the basis for $C(A)$  
$\Rightarrow$ $dim(C(A)) = 3$ and $rank(A) = 3$
 
 
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Matrices Factorizations
</ins>
</div> 

1. $A = LU$ from eimination
2. $A = QR$ from orthogonalization 
3. $S = Q\Lambda Q^{T}$ from eigenvectors of a symmetric matrix $S$
4. $A = X\Lambda X^{-1}$ diagonalizeds $A$ by eigenvector matrix $X$. $A$ is not symmetric
5. $A = U\Sigma V^{T}$ (orthogonal)(diagonal)(orthogonal) = Singular Value Decomposition

 
  
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Eigenvalues & Eigenvectors
</ins>
</div>  

$$Ax_i = \lambda_i x_i$$

where $i = 1,...,n$, $A$ is a n by n matrix, $\lambda$ are the eigenvalue of $A$ and $x$ are the eigenvectors of $A$

Why do we want this?

* $A^2x = A(Ax) = A(\lambda x) = \lambda^2 x$ $\Rightarrow$ we find out that eigenvectors $x$ of $A$ are also the eigenvectors of $A^2$ and the eigenvalues of $A^2$ are $\lambda^2$ where $\lambda$ are the eigenvalues of $A$. We can then also extend it to 
* $A^kx = \lambda^kx$
* $A^{-1}x = \frac{1}{\lambda}x$. It's always true because when $\lambda=0$, $A$ is not invertible.
* $e^{At}x = e^{\lambda t}x$


Matrix $B$ is **Similar** to $A$ if

$$B = M^{-1} A M$$
where $M$ can be any invertible matrix. **Then $B$ and $A$ have the same eigenvalues.** Eigenvectors are not the same.

proof:
$$
\begin{aligned}
By &= \lambda y \\
M^{-1}AMy &= \lambda y \\
AMy &= M\lambda y \\
A(My) &= \lambda(My)
\end{aligned}
$$
The eigenvalues are still $\lambda$ and the eigenvectors change to $My$

Some other properties

* Eigenvalue(A) + Eigenvalue(B) $\neq$ Eigenvalue(A+B)
* $\sum \lambda = trace(A)$ (trace is adding the diagonal values)
* $\prod \lambda = \text{det}(A)$
* $AX = X\Lambda$ $\Rightarrow$ $A = X\Lambda X{-1}$ where $\Lambda$ is the matrix with eigenvalues $\lambda$ at the diagonal.

**Symmetric matrix**:

* Eigenvalues are real
* Eigenvectors are orthogonal: $S = Q\Lambda Q^{-1}$


<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Positive Definite Matrices
</ins>
</div>

Positive Definite Matrix: Symmetric matrices that has positive eigenvalues

The following 5 tests can be used for determining if the mattrix is symmetric positive definite. All of them are testing the same property but from a different point of view, so fulfilling one test means all others are also fulfilled.

1. All $\lambda_i > 0$
2. Energy $X^TSX > 0$ (all $x \neq 0$) 
3. $S = A^TA$ (independent columns in $A$)
4. All leading determinants > 0
5. All pivots in elimination > 0

The following content shows these 5 tests for matrix $S$

\[ S = 
\begin{bmatrix}
3 & 4\\
4 & 6\\
\end{bmatrix}
\]

1. skip
2. given the matrix is 2 by 2, we can assume \[X=\begin{bmatrix}
x\\
y\\
\end{bmatrix}
\]
\[
\begin{bmatrix}
x & y\\
\end{bmatrix}
\begin{bmatrix}
3 & 4\\
4 & 6\\
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
\end{bmatrix} = 2x^2+6y^2+8xy
\]
    + This can be a loss function $f(x,y)$, and we need $S$ to be positive definite so that $f(x,y)>0$ (convex, or bowl shape). 
    + Gradient descent can be used for finding the minimum. 
    $$\bigtriangledown f = 
    \begin{bmatrix} 
    \frac{\partial f}{\partial x} \\
    \frac{\partial f}{\partial y} \\ 
    \end{bmatrix}$$
    + If the one eigenvalue is very large and the other is very small, then the gradient descent would look like zig-zaging down the mountain instead of going down by straight line.

3. skip
4. 1st leading determinant = 3, 2nd leading determinant = $(3 \cdot 6)-(4 \cdot 4) = 2$
5. The pivots are $3$ and $\frac{2}{3}$ \[ S = 
\begin{bmatrix}
3 & 4\\
4 & 6\\
\end{bmatrix} \sim \begin{bmatrix}
3 & 4\\
0 & \frac{2}{3}\\
\end{bmatrix}
\]


<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
### Positive Semidefinite Matrices
</ins>
</div>

1. All $\lambda_i \geq 0$
2. Energy $X^TSX \geq 0$ (all $x \neq 0$) 
3. $S = A^TA$ (dependent columns in allowed)
4. All leading determinants $\geq 0$
5. $r$ pivots in elimination > 0, $r \leq n$

example 1: 

\[ S' = 
\begin{bmatrix}
3 & 4\\
4 & \frac{16}{3}\\
\end{bmatrix}
\]

* The determinant $det(S') = 0$ $\Rightarrow$ it has a 0 eigenvalue
* but how do we know the other eigenvalue is 0? By **trace**, Sum of $\lambda$s must be $3+\frac{16}{3} = \frac{25}{3}$. Therefore $\lambda$s must be $0$ and $\frac{25}{3}$

example 2:

\[
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix}
\]

It is a positive semidefinite matrix, with the eigenvalues 3,0,0 because:

* It is a singular matrix
* with rank=1 $\Rightarrow$ there is only 1 non-zero eigenvalue
* The trace is 3 $\Rightarrow$ the only non-zero eigenvalue is 3

Now, we try to decompose the matrix to $A^TA$

\[
\begin{aligned}
\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{bmatrix} &= \lambda_1q_1q_1^T+\lambda_2q_2q_2^T+\lambda_3q_3q_3^T = Q\Lambda Q^T \\
&= 3 \begin{bmatrix}
1/\sqrt{3} \\
1/\sqrt{3} \\
1/\sqrt{3} \\
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}}\\
\end{bmatrix} + 0 + 0
\end{aligned}
\]


Since a covariance matrix is derived by $A^TA$, it is by definition a positive semidefinite a matrix. However, there are circumstances that the "covariance/correlation matrix" is not positive semidefinite when they are not derive by $A^TA$

* **pairwise estimation**: It is common that when there are missing values in the sample data, analyst would estimate the covariance matrix by pairwise estimation, which is easy to do in R.
* **pairwise forecast**: The analyst can also forecast pairwise correlation of more than 2 variables, then in this case the matrix is not positive semidefinite and is not technically a covariance/correlation matrix.



