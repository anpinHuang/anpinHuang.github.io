---
title: "Credit Default Prediction"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
bibliography: references.bib
link-citations: yes
---

###  1. Overview

The goal of this page is to explore how different machine algorithms work for predicting the default probability of a credit position (loan).

The datasets that are being used are

* ***Application data***: The information of the loan application, including the borrower's income, the credit amount, the annuity (yearly payment), income source and etc.
* ***Credit bureau data***: The information from the credit bureau regarding the loan applicants, e.g. credit inquiry, days past due information.
* ***Credit card information***: The credit card information that the borrower's has with the lender.

The following machine learning algorithm are used:

* Logistic regression with WOEs
* Desicion tree
* Gradient boosting

Before building the model, please note the following limitations

* If the interest rate to the clients are not competitative compared to the peer, the model might suffer from **adverse selection**. Therefore, if the interest rates that this company offers decrease in general, the charasteristic of the applicants might change, and thus impact the representitativeness of the historical data
* If the model includes variables that are controlled by the company (**strategy-driven**), then a strategy change could impact the relationship between the default rate & the explanatory variable. This is again applicable to the interest rate charged to the loan applicant. Since the interest rate is a function of both the borrower's characteristic and the lender's risk appetite. If the company is able to decrease the interest charge because it just obtained a cheaper source of capital, then the model might "think" the new origination that bears lower interest rate to have better risk characteristic (lower risk) when the risk characteristics are exact the same as before.
* The **period** when the default occured is not in the dataset. Therefore, the calibrated default probability can only be considered as "through-the-cycle" probability of default.
* The **age** of the loan when it defaults is not in the dataset. Therefore, so important insights such as the expect credit loss, or the value of the loan cannot be measured.

```{r load-data, echo=FALSE, eval = FALSE}
tran <- read.csv("/Users/smartboyben/documents/Data/home-credit-default-risk/application_train.csv")
bureau <- read.csv("/Users/smartboyben/documents/Data/home-credit-default-risk/bureau.csv") 
card <- read.csv("/Users/smartboyben/documents/Data/home-credit-default-risk/credit_card_balance.csv")  

## randomly assign training & testing set
train_size <- round(nrow(tran)/3*2)
tran_train <- sample(1:nrow(tran), train_size, replace=FALSE)
tran_test <- setdiff(c(1:nrow(tran)), tran_train)
```

The bar-plot below shows the percentage of defaults (label=1) compared to the percentage of non-defaults (label=0).

```{r explore-dependent}
par(mfrow=c(1,2))
barplot(prop.table(table(tran[tran_train,"TARGET"])), main = "default flag in training set")
barplot(prop.table(table(tran[tran_test,"TARGET"])), main = "default flag in testing set")
```

### 2. Data treatment

```{r treat-data}
# 365243 should be considered as NA
tran[which(tran[,'DAYS_EMPLOYED']==365243),'DAYS_EMPLOYED'] <- NA
tran[which(tran[,'DAYS_BIRTH']==365243),'DAYS_BIRTH'] <- NA
```

```{r outlier}
#quantile(tran[,"AMT_INCOME_TOTAL"], probs = c(0.9,0.99))
mean_income <- mean(tran[,"AMT_INCOME_TOTAL"], na.rm = TRUE)
sd_income <- sd(tran[,"AMT_INCOME_TOTAL"], na.rm = TRUE)
## winsoring
upper_bound <- mean_income+3*sd_income
outlier <- nrow((tran[which(tran[,"AMT_INCOME_TOTAL"]>upper_bound),]))/nrow(tran)
tran$AMT_INCOME_TOTAL[tran$AMT_INCOME_TOTAL>upper_bound] <- upper_bound

cat(paste0("income average: ", round(mean_income), "\n", "income standard deviation: ", round(sd_income),"\n","percentage of income are winsorized: ", outlier,"\n"))
```

### 3. Feature engineering

* inverse of loan to value ratio (LTV) 
* inverse of loan to income ratio
* income to annuity ratio

```{r easy-features}
tran$VTL <- tran$AMT_GOODS_PRICE/tran$AMT_CREDIT
tran$ITL <- tran$AMT_INCOME_TOTAL/tran$AMT_CREDIT
tran$ITA <- tran$AMT_INCOME_TOTAL/tran$AMT_ANNUITY
```

Derive features from credit bureau data

* number of credit applications in the past 12 months
* number of credit applications in the past 24 months
* number of credit applications
* total outstanding credit
* active annuity from bureau (later use to calculate other ratios)
* utilization rate (credit/credit_limit)
```{r bureau-data}
library(sqldf)
 
bureau_groupbyID <- sqldf("
select SK_ID_CURR
, sum(case when DAYS_CREDIT > -365 then 1 else 0 end) as b_CreditInquiry12M
, sum(case when DAYS_CREDIT > -730 then 1 else 0 end) as b_CreditInquiry24M
, count(*) as b_CreditInquiryAll
, sum(case when CREDIT_ACTIVE='Active' then AMT_CREDIT_SUM else 0 end) as b_AMT_CREDIT_SUM
, sum(case when CREDIT_ACTIVE='Active' then AMT_CREDIT_SUM_DEBT else 0 end) as b_AMT_CREDIT_SUM_DEBT
, sum(case when CREDIT_ACTIVE='Active' then AMT_ANNUITY else 0 end) as b_AMT_ANNUITY
, sum(case when CREDIT_ACTIVE='Active' then AMT_CREDIT_SUM_LIMIT else 0 end) as b_AMT_CREDIT_SUM_LIMIT
from bureau 
group by 1")
```


Derive features from credit card balance

* average of the balance
* standard of the balance
* average of the limit
* standard deviation of the limit
* average of the utilization rate
* standard deviation of the utilication rate
* z-score of the last utilization rate
```{r credit-card-data}
library(sqldf)
## combined card information if 1 customer has more than 1 credit card
CardPerPerson <- sqldf("
select SK_ID_CURR, MONTHS_BALANCE
, sum(AMT_BALANCE) as AMT_BALANCE
, sum(AMT_CREDIT_LIMIT_ACTUAL) as AMT_CREDIT_LIMIT_ACTUAL
, max(SK_DPD) as SK_DPD
from card
group by 1,2")


CardFeatures <- sqldf("
select SK_ID_CURR
, avg(AMT_BALANCE) as c_AMT_BALANCE_mean
, stdev(AMT_BALANCE) as c_AMT_BALANCE_sd
, avg(AMT_CREDIT_LIMIT_ACTUAL) as c_AMT_LIMIT_mean
, stdev(AMT_CREDIT_LIMIT_ACTUAL) as c_AMT_LIMIT_sd
, avg(AMT_BALANCE/AMT_CREDIT_LIMIT_ACTUAL) c_utilization_mean
, stdev(AMT_BALANCE/AMT_CREDIT_LIMIT_ACTUAL) c_utilization_sd
, max(SK_DPD) as c_SK_DPD_max
, max(MONTHS_BALANCE) as last_month
from CardPerPerson
group by 1
")

CardFeatures <- sqldf("
select 
a.*
, (b.AMT_BALANCE/b.AMT_CREDIT_LIMIT_ACTUAL-a.c_utilization_mean)/a.c_utilization_sd as c_last_utl_zscore
from CardFeatures a
left join CardPerPerson b
on a.last_month = b.MONTHS_BALANCE
and a.SK_ID_CURR = b.SK_ID_CURR
")
```


Combine the features of credit bureau & credit card to the applicatino dataset
```{r combine-RDS}
tran_ext <- sqldf("
select 
a.*
, b.b_CreditInquiry12M
, b.b_CreditInquiry24M
, b.b_CreditInquiryAll
, b.b_AMT_CREDIT_SUM
, b.b_AMT_CREDIT_SUM_DEBT
, b.b_AMT_ANNUITY
, b.b_AMT_CREDIT_SUM_LIMIT
, c.c_AMT_BALANCE_mean
, c.c_AMT_BALANCE_sd
, c.c_AMT_LIMIT_mean
, c.c_AMT_LIMIT_sd
, c.c_utilization_mean
, c.c_utilization_sd
, c.c_last_utl_zscore
, c.c_SK_DPD_max
from tran a
left join bureau_groupbyID b
on a.SK_ID_CURR=b.SK_ID_CURR
left join CardFeatures c
on a.SK_ID_CURR=c.SK_ID_CURR")
```

Derive additional features
```{r more-features}
tran_ext$CreditIncreaseMult <- tran_ext$b_AMT_CREDIT_SUM/tran_ext$AMT_CREDIT
tran_ext$PaymentStress <- (tran_ext$b_AMT_ANNUITY + tran_ext$AMT_ANNUITY)/tran_ext$AMT_INCOME_TOTAL
tran_ext$InquiryRatio12M <- tran_ext$b_CreditInquiry12M/tran_ext$b_CreditInquiryAll
tran_ext$DaysEmployedRatio <- tran_ext$DAYS_EMPLOYED/tran_ext$DAYS_BIRTH
tran_ext$IncomeToDaysEmployed <- tran_ext$AMT_INCOME_TOTAL/tran_ext$DAYS_EMPLOYED
tran_ext$IncomePerPerson <- tran_ext$AMT_INCOME_TOTAL/tran_ext$CNT_FAM_MEMBERS

```

Data treatment again
```{r DQ}
tran_ext$c_utilization_mean[tran_ext$c_utilization_mean>1] <- 1

```


Exclude building variables due to large amount of missing
```{r}
exclude_list <- c("YEARS_BUILD_AVG"
,"COMMONAREA_AVG","ELEVATORS_AVG","ENTRANCES_AVG","FLOORSMAX_AVG","FLOORSMIN_AVG","LANDAREA_AVG","LIVINGAPARTMENTS_AVG","LIVINGAREA_AVG","NONLIVINGAPARTMENTS_AVG","NONLIVINGAREA_AVG","APARTMENTS_MODE","BASEMENTAREA_MODE","YEARS_BEGINEXPLUATATION_MODE","YEARS_BUILD_MODE","COMMONAREA_MODE","ELEVATORS_MODE","ENTRANCES_MODE","FLOORSMAX_MODE","FLOORSMIN_MODE","LANDAREA_MODE","LIVINGAPARTMENTS_MODE","LIVINGAREA_MODE","NONLIVINGAPARTMENTS_MODE","NONLIVINGAREA_MODE","APARTMENTS_MEDI","BASEMENTAREA_MEDI","YEARS_BEGINEXPLUATATION_MEDI","COMMONAREA_MEDI","ELEVATORS_MEDI","ENTRANCES_MEDI","FLOORSMAX_MEDI","FLOORSMIN_MEDI","LANDAREA_MEDI","LIVINGAPARTMENTS_MEDI","LIVINGAREA_MEDI","NONLIVINGAPARTMENTS_MEDI","NONLIVINGAREA_MEDI","FONDKAPREMONT_MODE","TOTALAREA_MODE","WALLSMATERIAL_MODE")
tran_ext <- tran_ext[,setdiff(colnames(tran_ext),exclude_list)]
```


### 4. Exploratory Data Analysis


```{r explore-independent1}
ocu_type <- sqldf("
select OCCUPATION_TYPE
, count(*) as ocu_count
from tran
group by 1
order by ocu_count desc")
library(knitr)
t1 <- head(head(ocu_type,10))
kable(t1, caption = 'Occupation type')
barplot(prop.table(table(tran[tran_train,"OCCUPATION_TYPE"])),las=2, main='Occupation type',cex.names=.5)
barplot(prop.table(table(tran[tran_train,"NAME_INCOME_TYPE"])),las=2, main='Income type',cex.names=.5)
barplot(prop.table(table(tran[tran_train,"NAME_EDUCATION_TYPE"])),las=2, main='Education type',cex.names=.5)
par(mfrow=c(1,2))
hist(tran[tran_train,"DAYS_EMPLOYED"], main='Days employed')
hist(tran[tran_train,"DAYS_BIRTH"], main='Days birth')
```

```{r explore-independent2}
par(mfrow=c(2,2))
hist(tran[tran_train,"EXT_SOURCE_1"], main='External source 1')
hist(tran[tran_train,"EXT_SOURCE_2"], main='External source 2')
hist(tran[tran_train,"EXT_SOURCE_3"], main='External source 3')
```

```{r explore-independent3}
tran[which(tran[,"VTL"]>2),"VTL"] <- NA
par(mfrow=c(2,2))
hist(tran[tran_train,"AMT_INCOME_TOTAL"], main='Income')
hist(tran[tran_train,"AMT_CREDIT"], main='Loan amount')
hist(tran[tran_train,"AMT_GOODS_PRICE"], main="Goods price")
hist(tran[tran_train,"VTL"], main="Goods price to loan amount ratio", xlim=c(0.5,1), breaks = 35)
```

### 5. Information value
```{r IV-1, include=FALSE}
library(Information)
library(gridExtra)

### Ranking variables using penalized IV  
IV <- create_infotables(data=tran_ext[tran_train,],y="TARGET")
```

```{r IV-2, echo = FALSE}
## require "brew install freetype" in ios terminal
library(kableExtra)
IV_Summary <- IV$Summary
rownames(IV_Summary) <- NULL
kable(IV_Summary[c(1:10),]) %>%
  kable_styling(full_width = FALSE, position = "float_left")
kable(IV_Summary[c(11:20),]) %>%
  kable_styling(full_width = FALSE, position = "left")
```

```{r IV-3, echo = FALSE}
kable(IV_Summary[c(21:30),]) %>%
  kable_styling(full_width = FALSE, position = "float_left")
kable(IV_Summary[c(31:40),]) %>%
  kable_styling(full_width = FALSE, position = "left")
```


```{r, echo=FALSE,eval = FALSE}
library(Information)
library(gridExtra)
### Ranking variables using penalized IV  
IV$Tables$EXT_SOURCE_1
IV$Tables$EXT_SOURCE_2
IV$Tables$EXT_SOURCE_3

IV$Tables$DAYS_EMPLOYED
IV$Tables$DAYS_BIRTH
IV$Tables$OCCUPATION_TYPE
IV$Tables$NAME_INCOME_TYPE
IV$Tables$NAME_INCOME_TYPE
#grid.table(IV$Tables$EXT_SOURCE_1, rows=NULL)
```


```{r IV-EDA-ggplot}
library(plyr)
library(ggplot2)
library(gridExtra)
tran_ext$TARGET_char <- as.character(tran_ext$TARGET)
mu_EXT_SOURCE_3 <- ddply(tran_ext, "TARGET_char", summarise, grp.mean=mean(EXT_SOURCE_3, na.rm = TRUE))
mu_DAYS_EMPLOYED <- ddply(tran_ext, "TARGET_char", summarise, grp.mean=mean(DAYS_EMPLOYED, na.rm = TRUE))
mu_b_CreditInquiry12M <- ddply(tran_ext, "TARGET_char", summarise, grp.mean=mean(b_CreditInquiry12M, na.rm = TRUE))
mu_c_utilization_mean <- ddply(tran_ext, "TARGET_char", summarise, grp.mean=mean(c_utilization_mean, na.rm = TRUE))


p_EXT_SOURCE_3 <- ggplot(tran_ext[tran_train,], aes(x=EXT_SOURCE_3, fill=TARGET_char, color=TARGET_char)) +
  geom_histogram(aes(y = ..density..), position="identity", alpha=0.5,na.rm = TRUE) +
  scale_color_manual(values=c('blue','red'))+
  scale_fill_manual(values=c('blue','red'))+
  theme(legend.position="bottom")+
  geom_vline(data=mu_EXT_SOURCE_3, aes(xintercept=grp.mean, color=TARGET_char),
             linetype="dashed")

p_DAYS_EMPLOYED <- ggplot(tran_ext[tran_train,], aes(x=DAYS_EMPLOYED, fill=TARGET_char, color=TARGET_char)) +
  geom_histogram(aes(y = ..density..), position="identity", alpha=0.5,na.rm = TRUE) +
  xlim(c(-15000, 0))+
  scale_color_manual(values=c('blue','red'))+
  scale_fill_manual(values=c('blue','red'))+
  theme(legend.position="bottom")+
  geom_vline(data=mu_DAYS_EMPLOYED, aes(xintercept=grp.mean, color=TARGET_char),
             linetype="dashed")

p_b_CreditInquiry12M <- ggplot(tran_ext[tran_train,], aes(x=b_CreditInquiry12M, fill=TARGET_char, color=TARGET_char)) +
  geom_histogram(aes(y = ..density..), position="identity", alpha=0.5,na.rm = TRUE) +
  xlim(c(0, 10))+
  scale_color_manual(values=c('blue','red'))+
  scale_fill_manual(values=c('blue','red'))+
  theme(legend.position="bottom")+
  geom_vline(data=mu_b_CreditInquiry12M, aes(xintercept=grp.mean, color=TARGET_char),
             linetype="dashed")

p_c_utilization_mean <- ggplot(tran_ext[tran_train,], aes(x=c_utilization_mean, fill=TARGET_char, color=TARGET_char)) +
  geom_histogram(aes(y = ..density..), position="identity", alpha=0.5, na.rm = TRUE) +
  scale_color_manual(values=c('blue','red'))+
  scale_fill_manual(values=c('blue','red'))+
  theme(legend.position="bottom")+
  geom_vline(data=mu_c_utilization_mean, aes(xintercept=grp.mean, color=TARGET_char),
             linetype="dashed")


grid.arrange(p_EXT_SOURCE_3, p_DAYS_EMPLOYED, nrow = 1)
grid.arrange(p_b_CreditInquiry12M, p_c_utilization_mean, nrow = 1)

```

###  6. Correlation analysis 

To avoid including strongly correlated variables in the explanatory variable set, the following correlation analysis is performed. 

```{r correlation, echo=FALSE,eval = FALSE}
## correlation analysis of X's

cor(tran_ext[tran_train,c('EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3')], use="pairwise.complete.obs",method="spearman")
cor(tran_ext[tran_train,c('DAYS_BIRTH','DAYS_EMPLOYED','IncomeToDaysEmployed','DaysEmployedRatio')],method="spearman",use='pairwise.complete.obs')
cor(tran_ext[tran_train,c('InquiryRatio12M','b_CreditInquiry12M','b_CreditInquiry24M')],method="spearman",use='pairwise.complete.obs')

cor(tran_ext[tran_train,c('b_AMT_CREDIT_SUM_DEBT','CreditIncreaseMult','AMT_CREDIT','PaymentStress','b_AMT_ANNUITY')],method="spearman",use='pairwise.complete.obs')

cor(tran_ext[tran_train,c('c_utilization_mean','c_AMT_BALANCE_mean')],method="spearman",use='pairwise.complete.obs')
```

```{r correlation2}
library(ggplot2)
library(reshape2)
corX <- c('EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH','DAYS_EMPLOYED','IncomeToDaysEmployed','DaysEmployedRatio','InquiryRatio12M','b_CreditInquiry12M','b_CreditInquiry24M','b_AMT_CREDIT_SUM_DEBT','CreditIncreaseMult','AMT_CREDIT','PaymentStress','b_AMT_ANNUITY','c_utilization_mean','c_AMT_BALANCE_mean','c_last_utl_zscore')

### Get lower triangle of the correlation matrix
get_lower_tri <- function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}
### Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

corX_matrix <- cor(tran_ext[tran_train,corX],method="spearman",use='pairwise.complete.obs')
upper_tri <- round(abs(get_upper_tri(corX_matrix)),2)
melted_cormat <- melt(upper_tri, na.rm = TRUE)

### Heatmap
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 geom_text(aes(label = value),  color = 'black', size = 2) +
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(0,1), space = "Lab", 
   name="Spearman\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 8, hjust = 1))+
 coord_fixed()
```

### 7. Strategy-driven variables

Variables that are related to the loan amount (credit) or the payment (strategy) can be strategy-driven and thus are not ideal variables for predicting the default risk. For example, if the company's lending costs or operational costs have decreased and therefore can take on more risk and increase the credit it is extending. This change does not directly provide the casual effect on the default risk of the client. 


### 8. Logistic regression with WOEs

The explanatory variables are transformed to WOE before serving as the independent variables for the logistic regression. Highly correlated variables are excluded from the previous step.

```{r WOE, eval = FALSE}
library(scorecard)
vars_logistic <- c('EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','InquiryRatio12M','DAYS_BIRTH','DAYS_EMPLOYED','OCCUPATION_TYPE','VTL','ORGANIZATION_TYPE','NAME_INCOME_TYPE','AMT_GOODS_PRICE','NAME_EDUCATION_TYPE','c_utilization_mean','c_last_utl_zscore','b_CreditInquiry24M','REGION_RATING_CLIENT_W_CITY','b_AMT_CREDIT_SUM_DEBT','DAYS_LAST_PHONE_CHANGE','PaymentStress','HOUSETYPE_MODE','NAME_FAMILY_STATUS','b_AMT_ANNUITY','OWN_CAR_AGE','REG_CITY_NOT_LIVE_CITY','DAYS_REGISTRATION','CODE_GENDER','FLAG_EMP_PHONE','FLAG_DOCUMENT_3')

## the binning & woe is created using training set and will later be applied to testing set to create clean out-of-sample testing
woes <- woebin(dt = tran_ext[tran_train,c('TARGET',vars_logistic)], y='TARGET')

var_woes <- woebin_ply(dt = tran_ext[tran_train,c('TARGET',vars_logistic)], bins = woes, to = "woe", no_cores = 2, print_step = 0L)
```

```{r}
model <- glm(TARGET ~.,family=binomial(link='logit'),data=var_woes)
summary(model)
```


```{r, message=FALSE, include=FALSE}
library(pROC)
library(scorecard)
predicted_logistic <- predict(model, var_woes, type="response")

var_woes_test <- woebin_ply(dt = tran_ext[tran_test,c('TARGET',vars_logistic)], bins = woes, to = "woe", no_cores = 2, print_step = 0)
predicted_logistic_test <- predict(model, var_woes_test, type="response")
```

```{r, message=FALSE}
auc_logistic_train <- auc(tran_ext[tran_train,"TARGET"], predicted_logistic)
auc_logistic_test <- auc(tran_ext[tran_test,"TARGET"], predicted_logistic_test)
cat(paste0("Training AUC: ", auc_logistic_train,"\n", "Testing AUC: ", auc_logistic_test,"\n"))
```


### 9. Decision tree

```{r tree,eval = FALSE, include=FALSE}
library(rpart)
mytree <- rpart(TARGET~., data = tran[tran_train,], method = "class", cp=0.0001, maxdepth = 9)
```

```{r, message=FALSE}
library(rpart)
predicted_tree_train <- predict(mytree, tran[tran_train,], type ="prob")
auc_tree_train <- auc(tran[tran_train,"TARGET"], predicted_tree_train[,'1'])
predicted_tree_test <- predict(mytree, tran[tran_test,], type ="prob")
auc_tree_test <- auc(tran[tran_test,"TARGET"], predicted_tree_test[,'1'])  

cat(paste0("Training AUC: ", auc_tree_train,"\n", "Testing AUC: ", auc_tree_test,"\n"))                          
```


### 10. Gradient boosting

```{r,eval = FALSE, message=FALSE}
library(gbm)
gbm_tree <- gbm.fit <- gbm(
  formula = TARGET ~ .,
  distribution = "bernoulli",
  data = tran_ext[tran_train,],
  n.minobsinnode=30,
  n.trees = 1000,
  interaction.depth = 2,
  shrinkage = 0.005, # learning rate
  n.cores = NULL, # will use all cores by default
  verbose = TRUE
  )                                
                                
```

```{r, message=FALSE}
library(gbm)
predicted_gbm_train <- predict(gbm_tree, n.trees = gbm_tree$n.trees, tran_ext[tran_train,], type="response")                             
auc_gbm_train <- auc(tran_ext[tran_train,"TARGET"], predicted_gbm_train)   
predicted_gbm_test <- predict(gbm_tree, n.trees = gbm_tree$n.trees, tran_ext[tran_test,], type="response")  
auc_gbm_test <- auc(tran_ext[tran_test,"TARGET"], predicted_gbm_test)  

cat(paste0("Training AUC: ", auc_gbm_train,"\n", "Testing AUC: ", auc_gbm_test,"\n"))
```

### 11. Conclusion

to be continued...

```{r, echo=FALSE, eval = FALSE}
## code to create the html, run it in console
rmarkdown::render("/Users/smartboyben/documents/git/anpinHuang.github.io/HomeCredit.Rmd")
```

