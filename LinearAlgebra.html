<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Basic Linear Algebra</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">An-Pin Huang</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About me</a>
</li>
<li>
  <a href="econometrics.html">Econometrics</a>
</li>
<li>
  <a href="dataScienceML.html">Data Science</a>
</li>
<li>
  <a href="Finance.html">Financial Math &amp; Risk</a>
</li>
<li>
  <a href="Programming.html">Programming</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Basic Linear Algebra</h1>

</div>


<p>This page is a study notes of the course <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">Matrix Methods in Data Analysis, Signal Processing, and Machine Learning</a> by professor Gilbert Strang.</p>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="column-space-rank">Column Space &amp; Rank</h3>
</ins>
</div>
<ul>
<li>Column space<br />
<span class="math inline">\(C(A) = span(\overrightarrow{a_1},\overrightarrow{a_2},\overrightarrow{a_3},\overrightarrow{a_4},\overrightarrow{a_5})\)</span></li>
<li>Basis for <span class="math inline">\(C(A)\)</span>:<br />
vectors of span <span class="math inline">\(C(A)\)</span> that are linear independent</li>
</ul>
<p><span class="math display">\[A = 
\begin{bmatrix}
1 &amp; 0 &amp; -1 &amp; 0 &amp; 4 \\
2 &amp; 1 &amp; 0 &amp; 0 &amp; 9 \\
-1 &amp; 2 &amp; 5 &amp; 1&amp; -5\\
1 &amp; -1 &amp; -3 &amp; -2 &amp; 9 \\
\end{bmatrix}
\]</span></p>
<p><strong>Reduced row echelon form</strong> by row operation: <span class="math display">\[rref(A) = R = 
\begin{bmatrix}
1 &amp; 0 &amp; -1 &amp; 0 &amp; 4 \\
0 &amp; 1 &amp; 2 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 1&amp; -3\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}
\]</span></p>
<p>The <strong>pivot column</strong> <span class="math inline">\(\overrightarrow{r_1}\)</span>, <span class="math inline">\(\overrightarrow{r_2}\)</span>, and <span class="math inline">\(\overrightarrow{r_4}\)</span> of matix <span class="math inline">\(R\)</span> are linear independent<br />
<span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\overrightarrow{a_1}\)</span>, <span class="math inline">\(\overrightarrow{a_2}\)</span>, and <span class="math inline">\(\overrightarrow{a_4}\)</span> are also linear independent<br />
<span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\overrightarrow{a_1}\)</span>, <span class="math inline">\(\overrightarrow{a_2}\)</span>, and <span class="math inline">\(\overrightarrow{a_4}\)</span> form the basis for <span class="math inline">\(C(A)\)</span><br />
<span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(dim(C(A)) = 3\)</span> and <span class="math inline">\(rank(A) = 3\)</span></p>
<p><strong>Understanding the matrix multiplication</strong></p>
<p><span class="math display">\[A = 
\begin{bmatrix}
2 &amp; 1 &amp; 3\\
3 &amp; 1 &amp; 4\\
5 &amp; 7 &amp; 12\\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
= x_1 \begin{bmatrix}
2 \\
3 \\
5 \\
\end{bmatrix}
+x_2 \begin{bmatrix}
1 \\
1 \\
7 \\
\end{bmatrix}
+ x_3 \begin{bmatrix}
3 \\
4 \\
12 \\
\end{bmatrix}
\]</span></p>
<p><span class="math inline">\(A_{3x3} = C_{3x2} \ R_{2x3}\)</span> where <span class="math inline">\(C\)</span> is the column space of matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(R\)</span> is the row space of matrix <span class="math inline">\(A\)</span></p>
<p><span class="math display">\[
\begin{bmatrix}
2 &amp; 1 &amp; 3\\
3 &amp; 1 &amp; 4\\
5 &amp; 7 &amp; 12\\
\end{bmatrix}
= 
\begin{bmatrix}
2 &amp; 1\\
3 &amp; 1\\
5 &amp; 7\\
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; 1\\
0 &amp; 1 &amp; 1\\
\end{bmatrix}
\]</span></p>
<p><strong>column rank <span class="math inline">\(r=2\)</span> = row rank</strong></p>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="matrices-factorizations">Matrices Factorizations</h3>
</ins>
</div>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A = LU\)</span> from eimination we get the the upper matrix <span class="math inline">\(U\)</span> <span class="math display">\[
\begin{bmatrix}
2 &amp; 3\\
4 &amp; 7\\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
2 &amp; 3\\
0 &amp; 1\\
\end{bmatrix}
\]</span> then <span class="math inline">\(A = LU\)</span><br />
<span class="math display">\[
\begin{bmatrix}
2 &amp; 3\\
4 &amp; 7\\
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; 0\\
2 &amp; 1\\
\end{bmatrix}
\begin{bmatrix}
2 &amp; 3\\
0 &amp; 1\\
\end{bmatrix}
\]</span></li>
<li><span class="math inline">\(A = QR\)</span> from orthogonalization</li>
<li><span class="math inline">\(S = Q\Lambda Q^{T}\)</span> from eigenvectors of a symmetric matrix <span class="math inline">\(S\)</span></li>
<li><span class="math inline">\(A = X\Lambda X^{-1}\)</span> diagonalizeds <span class="math inline">\(A\)</span> by eigenvector matrix <span class="math inline">\(X\)</span>. <span class="math inline">\(A\)</span> is not symmetric</li>
<li><span class="math inline">\(A = U\Sigma V^{T}\)</span> (orthogonal)(diagonal)(orthogonal) = Singular Value Decomposition</li>
</ol>
<p><strong>4 Fundamental Subspaces A m by n rank r</strong></p>
<ul>
<li>Column space <span class="math inline">\(C(A)\)</span>: dim = r</li>
<li>Row space <span class="math inline">\(C(A^T)\)</span>: dim = r (same as dim of column space)</li>
<li>Null space of the matrix <span class="math inline">\(N(A)\)</span>: dim = n-r</li>
<li>Null space of transpose of A <span class="math inline">\(N(A^T)\)</span>: dim = m-r</li>
<li><span class="math inline">\(C(A)\)</span> is orthogonal to <span class="math inline">\(N(A)\)</span> (1st column of <span class="math inline">\(A\)</span> is orthogonal to x, 2nd column of <span class="math inline">\(A\)</span> is orthogonal to x,…)</li>
<li><span class="math inline">\(C(A^T)\)</span> is orthogonal to <span class="math inline">\(N(A^T)\)</span></li>
</ul>
<p>Null space is all solutions of <span class="math inline">\(Ax = 0\)</span></p>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="orthonormal-orthogonal">Orthonormal &amp; Orthogonal</h3>
</ins>
</div>
<p><strong>Orthonormal Columns <span class="math inline">\(Q\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(Q^TQ = I\)</span></li>
<li><span class="math inline">\(QQ^T = I\)</span>? Sometimes yes, sometimes no. Yes when Q is a square matrix, and <span class="math inline">\(Q\)</span> is called orthogonal matix
<ul>
<li><strong>example 1</strong>: rotation matrix <span class="math display">\[Q = 
\begin{bmatrix}
cos \theta &amp;  -sin \theta\\
sin \theta &amp; cos \theta\\
\end{bmatrix}
\]</span></li>
<li><strong>example 2</strong>: reflection matrix <span class="math display">\[Q = 
\begin{bmatrix}
cos \theta &amp;  sin \theta\\
sin \theta &amp; -cos \theta\\
\end{bmatrix}
\]</span></li>
</ul></li>
</ul>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="eigenvalues-eigenvectors">Eigenvalues &amp; Eigenvectors</h3>
</ins>
</div>
<p><span class="math display">\[Ax_i = \lambda_i x_i\]</span></p>
<p>where <span class="math inline">\(i = 1,...,n\)</span>, <span class="math inline">\(A\)</span> is a n by n matrix, <span class="math inline">\(\lambda\)</span> are the eigenvalue of <span class="math inline">\(A\)</span> and <span class="math inline">\(x\)</span> are the eigenvectors of <span class="math inline">\(A\)</span></p>
<p>Why do we want this?</p>
<ul>
<li><span class="math inline">\(A^2x = A(Ax) = A(\lambda x) = \lambda^2 x\)</span> <span class="math inline">\(\Rightarrow\)</span> we find out that eigenvectors <span class="math inline">\(x\)</span> of <span class="math inline">\(A\)</span> are also the eigenvectors of <span class="math inline">\(A^2\)</span> and the eigenvalues of <span class="math inline">\(A^2\)</span> are <span class="math inline">\(\lambda^2\)</span> where <span class="math inline">\(\lambda\)</span> are the eigenvalues of <span class="math inline">\(A\)</span>. We can then also extend it to</li>
<li><span class="math inline">\(A^kx = \lambda^kx\)</span></li>
<li><span class="math inline">\(A^{-1}x = \frac{1}{\lambda}x\)</span>. It’s always true because when <span class="math inline">\(\lambda=0\)</span>, <span class="math inline">\(A\)</span> is not invertible.</li>
<li><span class="math inline">\(e^{At}x = e^{\lambda t}x\)</span></li>
</ul>
<p>Matrix <span class="math inline">\(B\)</span> is <strong>Similar</strong> to <span class="math inline">\(A\)</span> if</p>
<p><span class="math display">\[B = M^{-1} A M\]</span> where <span class="math inline">\(M\)</span> can be any invertible matrix. <strong>Then <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> have the same eigenvalues.</strong> Eigenvectors are not the same.</p>
<p>proof: <span class="math display">\[
\begin{aligned}
By &amp;= \lambda y \\
M^{-1}AMy &amp;= \lambda y \\
AMy &amp;= M\lambda y \\
A(My) &amp;= \lambda(My)
\end{aligned}
\]</span> The eigenvalues are still <span class="math inline">\(\lambda\)</span> and the eigenvectors change to <span class="math inline">\(My\)</span></p>
<p>Some other properties</p>
<ul>
<li>Eigenvalue(A) + Eigenvalue(B) <span class="math inline">\(\neq\)</span> Eigenvalue(A+B)</li>
<li><span class="math inline">\(\sum \lambda = trace(A)\)</span> (trace is adding the diagonal values)</li>
<li><span class="math inline">\(\prod \lambda = \text{det}(A)\)</span></li>
<li><span class="math inline">\(AX = X\Lambda\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(A = X\Lambda X{-1}\)</span> where <span class="math inline">\(\Lambda\)</span> is the matrix with eigenvalues <span class="math inline">\(\lambda\)</span> at the diagonal.</li>
</ul>
<p><strong>Symmetric matrix</strong>:</p>
<ul>
<li>Eigenvalues are real</li>
<li>Eigenvectors are orthogonal: <span class="math inline">\(S = Q\Lambda Q^{-1}\)</span></li>
</ul>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="positive-definite-matrices">Positive Definite Matrices</h3>
</ins>
</div>
<p>Positive Definite Matrix: Symmetric matrices that has positive eigenvalues</p>
<p>The following 5 tests can be used for determining if the mattrix is symmetric positive definite. All of them are testing the same property but from a different point of view, so fulfilling one test means all others are also fulfilled.</p>
<ol style="list-style-type: decimal">
<li>All <span class="math inline">\(\lambda_i &gt; 0\)</span></li>
<li>Energy <span class="math inline">\(X^TSX &gt; 0\)</span> (all <span class="math inline">\(x \neq 0\)</span>)</li>
<li><span class="math inline">\(S = A^TA\)</span> (independent columns in <span class="math inline">\(A\)</span>)</li>
<li>All leading determinants &gt; 0</li>
<li>All pivots in elimination &gt; 0</li>
</ol>
<p>The following content shows these 5 tests for matrix <span class="math inline">\(S\)</span></p>
<p><span class="math display">\[ S = 
\begin{bmatrix}
3 &amp; 4\\
4 &amp; 6\\
\end{bmatrix}
\]</span></p>
<ol style="list-style-type: decimal">
<li>skip</li>
<li>given the matrix is 2 by 2, we can assume <span class="math display">\[X=\begin{bmatrix}
x\\
y\\
\end{bmatrix}
\]</span> <span class="math display">\[
\begin{bmatrix}
x &amp; y\\
\end{bmatrix}
\begin{bmatrix}
3 &amp; 4\\
4 &amp; 6\\
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
\end{bmatrix} = 2x^2+6y^2+8xy
\]</span>
<ul>
<li>This can be a loss function <span class="math inline">\(f(x,y)\)</span>, and we need <span class="math inline">\(S\)</span> to be positive definite so that <span class="math inline">\(f(x,y)&gt;0\)</span> (convex, or bowl shape).</li>
<li>Gradient descent can be used for finding the minimum. <span class="math display">\[\bigtriangledown f = 
 \begin{bmatrix} 
 \frac{\partial f}{\partial x} \\
 \frac{\partial f}{\partial y} \\ 
 \end{bmatrix}\]</span></li>
<li>If the one eigenvalue is very large and the other is very small, then the gradient descent would look like zig-zaging down the mountain instead of going down by straight line.</li>
</ul></li>
<li>skip</li>
<li>1st leading determinant = 3, 2nd leading determinant = <span class="math inline">\((3 \cdot 6)-(4 \cdot 4) = 2\)</span></li>
<li>The pivots are <span class="math inline">\(3\)</span> and <span class="math inline">\(\frac{2}{3}\)</span> <span class="math display">\[ S = 
\begin{bmatrix}
3 &amp; 4\\
4 &amp; 6\\
\end{bmatrix} \sim \begin{bmatrix}
3 &amp; 4\\
0 &amp; \frac{2}{3}\\
\end{bmatrix}
\]</span></li>
</ol>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="positive-semidefinite-matrices">Positive Semidefinite Matrices</h3>
</ins>
</div>
<ol style="list-style-type: decimal">
<li>All <span class="math inline">\(\lambda_i \geq 0\)</span></li>
<li>Energy <span class="math inline">\(X^TSX \geq 0\)</span> (all <span class="math inline">\(x \neq 0\)</span>)</li>
<li><span class="math inline">\(S = A^TA\)</span> (dependent columns in allowed)</li>
<li>All leading determinants <span class="math inline">\(\geq 0\)</span></li>
<li><span class="math inline">\(r\)</span> pivots in elimination &gt; 0, <span class="math inline">\(r \leq n\)</span></li>
</ol>
<p>example 1:</p>
<p><span class="math display">\[ S&#39; = 
\begin{bmatrix}
3 &amp; 4\\
4 &amp; \frac{16}{3}\\
\end{bmatrix}
\]</span></p>
<ul>
<li>The determinant <span class="math inline">\(det(S&#39;) = 0\)</span> <span class="math inline">\(\Rightarrow\)</span> it has a 0 eigenvalue</li>
<li>but how do we know the other eigenvalue is 0? By <strong>trace</strong>, Sum of <span class="math inline">\(\lambda\)</span>s must be <span class="math inline">\(3+\frac{16}{3} = \frac{25}{3}\)</span>. Therefore <span class="math inline">\(\lambda\)</span>s must be <span class="math inline">\(0\)</span> and <span class="math inline">\(\frac{25}{3}\)</span></li>
</ul>
<p>example 2:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{bmatrix}
\]</span></p>
<p>It is a positive semidefinite matrix, with the eigenvalues 3,0,0 because:</p>
<ul>
<li>It is a singular matrix</li>
<li>with rank=1 <span class="math inline">\(\Rightarrow\)</span> there is only 1 non-zero eigenvalue</li>
<li>The trace is 3 <span class="math inline">\(\Rightarrow\)</span> the only non-zero eigenvalue is 3</li>
</ul>
<p>Now, we try to decompose the matrix to <span class="math inline">\(A^TA\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{bmatrix} &amp;= \lambda_1q_1q_1^T+\lambda_2q_2q_2^T+\lambda_3q_3q_3^T = Q\Lambda Q^T \\
&amp;= 3 \begin{bmatrix}
1/\sqrt{3} \\
1/\sqrt{3} \\
1/\sqrt{3} \\
\end{bmatrix}
\begin{bmatrix}
\frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{3}} &amp; \frac{1}{\sqrt{3}}\\
\end{bmatrix} + 0 + 0
\end{aligned}
\]</span></p>
<p>Since a covariance matrix is derived by <span class="math inline">\(A^TA\)</span>, it is by definition a positive semidefinite a matrix. However, there are circumstances that the “covariance/correlation matrix” is not positive semidefinite when they are not derive by <span class="math inline">\(A^TA\)</span></p>
<ul>
<li><strong>pairwise estimation</strong>: It is common that when there are missing values in the sample data, analyst would estimate the covariance matrix by pairwise estimation, which is easy to do in R.</li>
<li><strong>pairwise forecast</strong>: The analyst can also forecast pairwise correlation of more than 2 variables, then in this case the matrix is not positive semidefinite and is not technically a covariance/correlation matrix.</li>
</ul>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="singular-value-decomposition">Singular Value Decomposition</h3>
</ins>
</div>
<p>Compared with <span class="math inline">\(S = Q\Lambda Q^T\)</span>.<br />
Now <span class="math inline">\(A = U\Sigma V^T\)</span>. <span class="math inline">\(Ax = U\Sigma V^Tx\)</span> can be viewed as <strong>rotation, stretch, and then rotation</strong> of vector <span class="math inline">\(x\)</span>.</p>
<p>Now the matrix <span class="math inline">\(A\)</span> is rectengular</p>
<p><span class="math inline">\(A^TA\)</span> is n by n semidefinite positive <span class="math inline">\(\rightarrow A^TA = V\Lambda V^T\)</span> <span class="math inline">\(AA^T\)</span> is m by m semidefinite positive <span class="math inline">\(\rightarrow AA^T = U\Lambda U^T\)</span></p>
<ul>
<li>Recall when looking for eigenvalue <span class="math inline">\(Ax = \lambda x\)</span></li>
<li><strong>Now for SVD, we are looking for a set of orthogonal vectors <span class="math inline">\(v\)</span> mutiply by <span class="math inline">\(A\)</span>, we get a set of orthogonal <span class="math inline">\(u\)</span> mutiply by singular values</strong> <span class="math inline">\(\sigma\)</span>: <span class="math inline">\(Av_i = \sigma u_i\)</span> for <span class="math inline">\(i = 1,...r\)</span> where <span class="math inline">\(r\)</span> is the rank<br />
<span class="math display">\[
\begin{aligned}
\rightarrow AV &amp;= U\Sigma \\
A &amp;= U\Sigma V^T \\
A^TA &amp;= V\Sigma^TU^TU\Sigma V^T \\
A^TA &amp;= V(\Sigma^T\Sigma)V^T
\end{aligned}
\]</span></li>
</ul>
<p>where</p>
<ul>
<li><span class="math inline">\(U\)</span> is the eigenvectors of <span class="math inline">\(AA^T\)</span> and</li>
<li><span class="math inline">\(V\)</span> is the eigenvectors of <span class="math inline">\(A^TA\)</span></li>
</ul>
<p>We know that vestors <span class="math inline">\(V\)</span> are orthogonal eigenvectors of <span class="math inline">\(A^TA\)</span>. We want to show <span class="math inline">\(u_1^Tu_2 =0\)</span></p>
<p><span class="math display">\[
\begin{aligned}
u_1^Tu_2 &amp;= (\frac{Av_1}{\sigma_1})^T(\frac{Av_2}{\sigma_2}) \\
&amp;= \frac{v_1^TA^TAv_2}{\sigma_1\sigma_2}\\
&amp;= \frac{v_1^T\sigma_2^2v_2}{\sigma_1\sigma_2}\\
&amp;= 0 \\
&amp;\rightarrow u_1,u_2 \text{ are also orthogonal}
\end{aligned}
\]</span></p>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="eckart-young">Eckart-Young</h3>
</ins>
</div>
<p><span class="math display">\[A = U\Sigma V^T = \sigma_1u_1v_1^T + \sigma_2u_2v_2^T + ... \sigma_ru_rv_r^T\]</span> <span class="math display">\[A_k = U_k\Sigma_k V_k^T = \sigma_1u_1v_1^T + \sigma_2u_2v_2^T + ... \sigma_ku_kv_k^T\]</span></p>
<p><strong>Eckart-Young Statement</strong>: If B has rank k then <span class="math inline">\(||A-B|| \geq||A-A_k||\)</span></p>
<p>About <strong>norms</strong> for vectors:</p>
<ul>
<li><span class="math inline">\(L^2\)</span> norm: <span class="math inline">\(||v||_2 = \sqrt{|v_1|^2+ ...+|v_n|^2}\)</span></li>
<li><span class="math inline">\(L^1\)</span> norm: <span class="math inline">\(||v||_1 = |v_1| +...+ |v_n|\)</span></li>
<li><span class="math inline">\(L^{\infty}\)</span> norm: <span class="math inline">\(||v||_{\infty} = \text{max}|v_i|\)</span></li>
<li><span class="math inline">\(||cv|| = |c| \ ||v||\)</span></li>
<li><span class="math inline">\(||v+w|| \leq ||v||+||w||\)</span></li>
</ul>
<p>Below <strong>norms</strong> for matrix <span class="math inline">\(A\)</span> satisfy Eckart-Young statement:</p>
<ul>
<li><span class="math inline">\(||A||_2 = \sigma_1\)</span></li>
<li>Frobenious norm: <span class="math inline">\(||A||_F = \sqrt{|a_{11}|^2+ ...|a_{1n}|^2+|a_{b1}|^2+...+|a_{mn}|^2}\)</span></li>
<li>Nuclear norm: <span class="math inline">\(||A||_N = \sigma_1+\sigma_2+...+\sigma_r\)</span></li>
</ul>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="norms-of-vectors-and-matrices">Norms of Vectors and Matrices</h3>
</ins>
</div>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="ways-to-solve-least-squares">4 Ways to Solve Least Squares</h3>
</ins>
</div>
<p><span class="math inline">\(Ax=b\)</span></p>
<p>where <span class="math inline">\(A\)</span> is <span class="math inline">\(m\)</span> by <span class="math inline">\(n\)</span> and rank <span class="math inline">\(r\)</span></p>
<ol style="list-style-type: decimal">
<li>Solve <span class="math inline">\(A^TAx = A^Tb\)</span> to minimize <span class="math inline">\(||Ax-b||^2\)</span> if <span class="math inline">\(A\)</span> has independent columns then <span class="math inline">\(A^TA\)</span> is invertible</li>
<li><p><strong>Gram-Schmidt</strong> <span class="math inline">\(A = QR\)</span> leads to <span class="math inline">\(x = R^{-1}Q^Tb\)</span></p></li>
<li><p>The pseudoinverse directly multiplies <span class="math inline">\(b\)</span> to give <span class="math inline">\(x = A^{+}b\)</span><br />
<span class="math inline">\(A = U\Sigma V^T\)</span>, if A is invertible, then <span class="math inline">\(A^{-1} = V\Sigma^{-1}U^T\)</span>. If A is not invertible: <span class="math inline">\(A^{+} = V\Sigma^{+}U^T\)</span></p></li>
<li><p>THe best <span class="math inline">\(x\)</span> is the limit of <span class="math inline">\((A^TA + \delta I)^{-1}A^Tb\)</span> as <span class="math inline">\(\delta \to 0\)</span></p></li>
</ol>
<p><strong>Claim: <span class="math inline">\(A^{+}b = (A^TA)^{-1}A^Tb\)</span> </strong> when <span class="math inline">\(N(A) = 0\)</span>, <span class="math inline">\(r=n\)</span></p>
<p><span class="math inline">\(A^{+} = V\Sigma^{+}U^T = (A^TA)^{-1}A^T\)</span></p>
<p>Notice that</p>
<ul>
<li><span class="math inline">\((A^TA)^{-1}A^TA=I\)</span></li>
<li>while <span class="math inline">\(A(A^TA)^{-1}A^T \neq I\)</span> Note that for <span class="math inline">\((A^TA)^{-1}A^T\)</span> to be the inverse of <span class="math inline">\(A\)</span>, we need <span class="math inline">\((A^TA)^{-1}A^TA=I\)</span> and <span class="math inline">\(A(A^TA)^{-1}A^T = I\)</span></li>
</ul>
<div style="background-color:white;color:blue;matrixOperation:20px;">
<ins>
<h3 id="difficulties-with-ax-b">Difficulties with Ax = b</h3>
</ins>
</div>
<p><span class="math inline">\(Ax=b\)</span></p>
<ol start="0" style="list-style-type: decimal">
<li><span class="math inline">\(x = A^{+}b\)</span> The <strong>pseudoinverse</strong></li>
<li>Sign ok, condition <span class="math inline">\(\frac{\sigma_1}{\sigma_n}\)</span> ok, x = A\b (matlab command of elimination)</li>
<li><span class="math inline">\(m&gt;n=\text{rank}\)</span>, then <span class="math inline">\((A^TA)^{-1} \hat{x}=A^Tb\)</span> the linear regression, where m is the number of equations</li>
<li><span class="math inline">\(m&lt;n\)</span>, then minimize the norm</li>
<li>Columns in bad condition -&gt; Gram-Schmidt normal or column pivoting</li>
<li>Near singular -&gt; inverse problem -&gt; add penality:<br />
<span class="math inline">\(\text{min} ||Ax-b||^2+\delta^2x^2\)</span><br />
</li>
<li></li>
</ol>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
