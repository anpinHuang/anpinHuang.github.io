---
title: "Decision tree"
---
Decision tree is a classification algorithm by dividing the data set into different subsets by the explanatory variables. The content presented in this section is mainly based on the knowledge learned from [Decision Tree Youtube tutorials playlist](https://www.youtube.com/playlist?list=PLBv09BD7ez_4temBw7vLA19p3tdQH6FYO) by [Victor Lavrenko](https://www.youtube.com/user/victorlavrenko/featured).


### Entropy
Entropy is a measure to measure the uncertainty of the outcome. If the outcome is certain (either all positive or all negative), then the entropy is 0. The highest entropy value is 1 when the probablity of outcome is 50-50.


$$ H(S) = - p{(+)}log_2p_{(+)} - p_{(-)}log_2p_{(-)} $$
impure (5 positive, 5 negative):
$$ H(S) = -\frac{5}{10}log_2 \frac{5}{10} - \frac{5}{10}log_2 \frac{5}{10} = 1  $$
pure set (5 positive, 0 negative):
$$ H(S) = -\frac{5}{5}log_2 \frac{5}{5} - \frac{0}{5}log_2 \frac{0}{5} = 0  $$
**$\Rightarrow$ The smaller the entropy the better!**

### Information Gain

$$  Gain(S,A) = H(S) - \sum \frac{|S_V|}{|S|}H(S_V) $$

where $V$ is the possible value of $A$, $S$ is the set of examples $X$ and $S_V$ is the subset where $X_A = V$

e.g. set $A$ has 10 positive and 6 negative and can be devided to subset $B$ (8 positive, 4 negative)and subset $C$ (2 positive, 2 negative)

Then, the entropy before the split is $H(S) = -\frac{10}{16}log_2 \frac{10}{16} - \frac{6}{16}log_2 \frac{6}{16} = 0.954434$

and the information gain after the split is:
$$ 
\begin{aligned}
IG &= H(S_A) - \frac{12}{16}H(S_B) - \frac{4}{16}H(S_C) \\
&= 0.954434-\frac{12}{16}0.9182958-\frac{4}{16}1 \\
&= 0.01571215 
\end{aligned}
$$
**The larger the IG (information gain) the better!**

### Problem of overfitting - bias towards multi-valued attributes. 
The algorithm will keep splitting data until each node is a pure set. (Assuming that the data is splitable. That is, there is no two data points with all x variables that are identical, but different y.) Hence the size of the tree will become too deep (with lots of layers and lots of nodes) and become too specific to the training data.   

Moreover, when there is a new data comes in and it is different from any of the historica data, then the algorithm doesn't know how to classify it.

### Gain ratio
There is one drawback of the information gain. That is, it tends to choose the feature that has many tiny subsets. The most extreme case is that if each data point has an unique ID, and then the entropy after spliting would be:
$$H(S) = \sum \frac{1}{n}0 = 0 $$
To penalize choosing a feature of a large number of distinct values, the Information gain ratio ($IGR$) is prefered than the information gain ($IG$).  
Information gain ratio:
$$ IGR = \frac{IG}{IV} $$
Where $IV$ is the Intrinsic value:
$$  IV(S,A) = -\sum \frac{|S_V|}{|S|}log_2\frac{|S_V|}{|S|} $$


### Continuous data
When the explanatory variable is continuous (numeric data), then it has to be transform to catergorical data by setting some thresold.

### Random forest
The random forest algorithm is aimed to address the shortcomings of the decision tree by training the data by a set of decision trees. For each tree, pick a subset of features at random to build the tree, and take the majority vote of all the trees.



