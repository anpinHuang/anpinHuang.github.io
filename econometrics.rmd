---
title: "OLS with Autocorrelation"
---

Assuming $y_t$ can be written as a linear function of $x$ and the error term $e$ has aurocorrelation
$$y_t = 1.2 + 2.2 x_{1,t} + 2.3 x_{2,t} + e_t $$
$$ e_t = \rho e_{t-1} + w_t ,\, where\, w \sim N(0,\sigma^2)$$
To simulate $y$ we do:
```{r}
library(MASS)
library(matrixStats)
m=30
x <- mvrnorm(n=m,mu=c(0.05,0.08),
             Sigma=matrix(c(0.0004,0.00023,0.00023,0.0006),
                          nrow=2,ncol=2,byrow=TRUE))
## assuming autocorrelation in x
et <- arima.sim(list(order = c(1,0,0),ar=0.6),n=m,rand.gen=rnorm,sd=0.1)
yt <- 1.2 + x %*% c(2.2,2.3)+et
```

Then we do regress $y$ on $x$ using OLS:
```{r}
regressY <- lm(yt~x)
summary(regressY)
regressY$coefficients
```
Test the autocorrelation of residuals
```{r}
resi <- regressY$residuals
pacf(resi)
ar.ols(resi)
```


Do GLS

$$y_t - \rho y_{t-1} = \alpha (1-\rho) + \beta_1(x_{1,t} - \rho  x_{1,t-1}) + \beta_2 (x_{2,t} - \rho  x_{2,t-1}) + w$$
```{r}
## regress on resi hat
regressE <- lm(resi[2:length(resi)]~resi[1:(length(resi)-1)]-1)
rho <- regressE$coefficients[1]
## GLS
## yt - rho * yt_1 = alpha * (1-rho) + beta1*(x1_t - rho * x1_t-1) + beta * 
## (x2_t - rho * x2_t-1)
yt2 <- yt[2:length(yt)]
xt2 <- x[2:nrow(x),]
xprime <- (xt2- c(rho,rho) * x[1:(nrow(x)-1),])
regressY_GLS <- lm( (yt2-rho*yt[1:(length(yt)-1)]) ~ xprime)
summary(regressY_GLS)
alpha <- regressY_GLS$coefficients[1]/(1-rho)
alpha
```

```{r eval=TRUE, echo=FALSE}
#Cochraneâ€“Orcutt iteration:
CochraneOrcuttIteration <- function(y,x,tolerance) {
  diff <- 1
  lastBetas <- rep(100,ncol(x)+1)
  lastRho <- 100
  regressY <- lm(y~x)
  resi <- regressY$residuals
  regressE <- lm(resi[2:length(resi)]~resi[1:(length(resi)-1)] -1)
  rho <- regressE$coefficients[1]
  yt2 <- as.matrix(y[2:length(y)])
  xt2 <- x[2:nrow(x),]
  while(diff > tolerance) {
  xprime <- (xt2- c(rho,rho) * x[1:(nrow(x)-1),])
  regressY_GLS <- lm( (yt2-rho*y[1:(length(y)-1)]) ~ xprime)
  alpha <- regressY_GLS$coefficients[1]/(1-rho)
  betas <- c(alpha,regressY_GLS$coefficients[2:length(regressY_GLS$coefficient)])
  resi <- y-cbind(1,x) %*% as.matrix(betas)
  regressE <- lm(resi[2:length(resi)]~resi[1:(length(resi)-1)])
  rho <- regressE$coefficients[2]
  diff <- abs(max(betas-lastBetas))
  diff <- max(diff,abs(rho-lastRho))
  lastBetas <- betas
  lastRho <- rho 
  }
  return(betas)
}
```
We redo the above steps for n times:
```{r}
n=2000
OLS_coefs <- matrix(nrow=n,ncol=3)
GLS_coefs <- matrix(nrow=n,ncol=3)
for(i in 1:n) {
  x <- mvrnorm(n=m,mu=c(0.05,0.08),
               Sigma=matrix(c(0.0004,0.00023,0.00023,0.0006),
                            nrow=2,ncol=2,byrow=TRUE))
  ## assuming autocorrelation in x
  et <- arima.sim(list(order = c(1,0,0),ar=0.6),n=m,rand.gen=rnorm,sd=0.1)
  yt <- 1.2 + x %*% c(2.2,2.3)+et
  
  regressY <- lm(yt~x)
  #summary(regressY)
  OLS_coefs[i,] <- regressY$coefficients
  
  ## GLS
  resi <- regressY$residuals
  regressE <- lm(resi[2:length(resi)]~resi[1:(length(resi)-1)])
  rho <- regressE$coefficients[2]
  ## GLS
  ## yt - rho * yt_1 = alpha * (1-rho) + beta1*(x1_t - rho * x1_t-1) + beta * 
  ## (x2_t - rho * x2_t-1)
  # yt2 <- yt[2:length(yt)]
  # xt2 <- x[2:nrow(x),]
  # xprime <- (xt2- c(rho,rho) * x[1:(nrow(x)-1),])
  # regressY_GLS <- lm( (yt2-rho*yt[1:(length(yt)-1)]) ~ xprime)
  # alpha <- regressY_GLS$coefficients[1]/(1-rho)
  # GLS_coefs[i,1] <- alpha
  # GLS_coefs[i,2:3] <- regressY_GLS$coefficients[2:3]
  GLS_coefs[i,] <- CochraneOrcuttIteration(yt,x,0.0005)
}
colMeans(OLS_coefs)
colMeans(GLS_coefs)
```
The variance of sampling distribution of GLS is much smaller than that of OLS, and is therefore more effieicnt.
```{r}
colVars(OLS_coefs)
colVars(GLS_coefs)

```
