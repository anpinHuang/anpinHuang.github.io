<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Decision tree</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">An-Pin Huang</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About me</a>
</li>
<li>
  <a href="econometrics.html">Econometrics</a>
</li>
<li>
  <a href="dataScienceML.html">Data Science</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Decision tree</h1>

</div>


<p>Decision tree is a classification algorithm by dividing the data set into different subsets by the explanatory variables. The content presented in this section is mainly based on the knowledge learned from <a href="https://www.youtube.com/playlist?list=PLBv09BD7ez_4temBw7vLA19p3tdQH6FYO">Decision Tree Youtube tutorials playlist</a> by <a href="https://www.youtube.com/user/victorlavrenko/featured">Victor Lavrenko</a>.</p>
<div id="entropy" class="section level3">
<h3>Entropy</h3>
<p>Entropy is a measure to measure the uncertainty of the outcome. If the outcome is certain (either all positive or all negative), then the entropy is 0. The highest entropy value is 1 when the probablity of outcome is 50-50.</p>
<p><span class="math display">\[ H(S) = - p{(+)}log_2p_{(+)} - p_{(-)}log_2p_{(-)} \]</span> impure (5 positive, 5 negative): <span class="math display">\[ H(S) = -\frac{5}{10}log_2 \frac{5}{10} - \frac{5}{10}log_2 \frac{5}{10} = 1  \]</span> pure set (5 positive, 0 negative): <span class="math display">\[ H(S) = -\frac{5}{5}log_2 \frac{5}{5} - \frac{0}{5}log_2 \frac{0}{5} = 0  \]</span> <strong><span class="math inline">\(\Rightarrow\)</span> The smaller the entropy the better!</strong></p>
<hr />
</div>
<div id="information-gain" class="section level3">
<h3>Information Gain</h3>
<p><span class="math display">\[  Gain(S,A) = H(S) - \sum \frac{|S_V|}{|S|}H(S_V) \]</span></p>
<p>where <span class="math inline">\(V\)</span> is the possible value of <span class="math inline">\(A\)</span>, <span class="math inline">\(S\)</span> is the set of examples <span class="math inline">\(X\)</span> and <span class="math inline">\(S_V\)</span> is the subset where <span class="math inline">\(X_A = V\)</span></p>
<p>e.g. set <span class="math inline">\(A\)</span> has 10 positive and 6 negative and can be devided to subset <span class="math inline">\(B\)</span> (8 positive, 4 negative)and subset <span class="math inline">\(C\)</span> (2 positive, 2 negative)</p>
<p>Then, the entropy before the split is <span class="math inline">\(H(S) = -\frac{10}{16}log_2 \frac{10}{16} - \frac{6}{16}log_2 \frac{6}{16} = 0.954434\)</span></p>
<p>and the information gain after the split is: <span class="math display">\[ 
\begin{aligned}
IG &amp;= H(S_A) - \frac{12}{16}H(S_B) - \frac{4}{16}H(S_C) \\
&amp;= 0.954434-\frac{12}{16}0.9182958-\frac{4}{16}1 \\
&amp;= 0.01571215 
\end{aligned}
\]</span> <strong>The larger the IG (information gain) the better!</strong></p>
<hr />
</div>
<div id="problem-of-overfitting---bias-towards-multi-valued-attributes." class="section level3">
<h3>Problem of overfitting - bias towards multi-valued attributes.</h3>
<p>The algorithm will keep splitting data until each node is a pure set. (Assuming that the data is splitable. That is, there is no two data points with all x variables that are identical, but different y.) Hence the size of the tree will become too deep (with lots of layers and lots of nodes) and become too specific to the training data.</p>
<p>Moreover, when there is a new data comes in and it is different from any of the historica data, then the algorithm doesn’t know how to classify it.</p>
<hr />
</div>
<div id="gain-ratio" class="section level3">
<h3>Gain ratio</h3>
<p>There is one drawback of the information gain. That is, it tends to choose the feature that has many tiny subsets. The most extreme case is that if each data point has an unique ID, and then the entropy after spliting would be: <span class="math display">\[H(S) = \sum \frac{1}{n}0 = 0 \]</span> To penalize choosing a feature of a large number of distinct values, the Information gain ratio (<span class="math inline">\(IGR\)</span>) is prefered than the information gain (<span class="math inline">\(IG\)</span>).<br />
Information gain ratio: <span class="math display">\[ IGR = \frac{IG}{IV} \]</span> Where <span class="math inline">\(IV\)</span> is the Intrinsic value: <span class="math display">\[  IV(S,A) = -\sum \frac{|S_V|}{|S|}log_2\frac{|S_V|}{|S|} \]</span></p>
<hr />
</div>
<div id="continuous-data" class="section level3">
<h3>Continuous data</h3>
<p>When the explanatory variable is continuous (numeric data), then it has to be transform to catergorical data by setting some thresold.</p>
<hr />
</div>
<div id="random-forest" class="section level3">
<h3>Random forest</h3>
<p>The random forest algorithm is aimed to address the shortcomings of the decision tree by training the data by a set of decision trees. For each tree, pick a subset of features at random to build the tree, and take the majority vote of all the trees.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
