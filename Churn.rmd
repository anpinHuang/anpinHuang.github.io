---
title: "Predicting Churns"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    includes:
      in_header: GA_Script.html
bibliography: references.bib
link-citations: yes
---

The purpose of this section is to use tree-based algorithm to predict churn of a customr based on some explanatory variable. The data I use is the Customer Support data from [IBM Watson Analytics](https://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/).

```{r, message=FALSE}
library(survival)
library(ggfortify)
library(rpart)
library(dplyr)
```

```{r}
churnAnalysis <- read.csv("WA_Fn-UseC_-Telco-Customer-Churn.csv") 
churnAnalysis <- as.data.frame(churnAnalysis)
head(churnAnalysis)
```
&nbsp;
&nbsp;
    
### Data pre-processing

Check if there is missing value. There are only 11 missing values in TotalCharges so we can just remove entire row given that the number is small.
```{r}
sapply(churnAnalysis, function(x) sum(is.na(x)))
churnAnalysis <- churnAnalysis[which(!is.na(churnAnalysis[,"TotalCharges"])),]
```

Then we look at what variables are continuous, and they are:
"tenure", "MonthlyCharges" and "TotalCharges"


### Kaplan Meier Analysis



```{r}
library(survival)
library(ggfortify)
churnAnalysis <- churnAnalysis %>% mutate(Churn_flag = ifelse(Churn=="Yes", 1, 0) )
km_churn <- with(churnAnalysis, Surv(tenure, Churn_flag))
km_fit <- survfit(Surv(tenure, Churn_flag) ~ 1, data=churnAnalysis)
autoplot(km_fit)
```


```{r}
km_fit_bill <- survfit(Surv(tenure, Churn_flag) ~ PaperlessBilling, data=churnAnalysis)
autoplot(km_fit_bill)
```

```{r}
km_fit_senior <- survfit(Surv(tenure, Churn_flag) ~ SeniorCitizen, data=churnAnalysis)
autoplot(km_fit_senior)
```


```{r}
km_fit_pay <- survfit(Surv(tenure, Churn_flag) ~ PaymentMethod, data=churnAnalysis)
autoplot(km_fit_pay)
```


### Information value 

### logistic regression with WOEs

### Decision tree

To use decision tree, we need to convert the continuous variables to categorical variables be setting some thresold. But first, let's split the data into training, validation and test sets.

```{r}
## randomize the data first
churnAnalysis <- churnAnalysis[sample(1:nrow(churnAnalysis)),]
## then split
churnAnalysisTraining <- churnAnalysis[1:(nrow(churnAnalysis)/2),]
churnAnalysisValidation <- churnAnalysis[(nrow(churnAnalysis)/2+1): (3*nrow(churnAnalysis)/4),]
churnAnalysisTest <- churnAnalysis[(3*nrow(churnAnalysis)/4+1),nrow(churnAnalysis),]
```


```{r}
hist(churnAnalysisTraining[,"MonthlyCharges"],breaks=40)
MonthlyChargesClass <- ifelse(churnAnalysis[,"MonthlyCharges"]<=30,"<=$30",">30$")
```
As the distribtion of "MonthlyCharges" does look like normal distribution and there seems to be a spike for monthly charges under 30. Hence, I try to split the data into 2:    
1. monthly charges $ \leq 30  $    
2. monthly charges $ > 30 $  
    

```{r}
hist(churnAnalysisTraining[,"TotalCharges"],breaks=40)
TotalChargesClass <- ifelse(churnAnalysis[,"TotalCharges"]<=500,"<=$500",">500$")
```

```{r}
hist(churnAnalysisTraining[,"tenure"],breaks=80)
TenureClass <- churnAnalysis[,"tenure"]
TenureClass[which(churnAnalysis[,"tenure"] >=70)] <- ">=70"
TenureClass[which(churnAnalysis[,"tenure"] <=5)] <- "<=5"
TenureClass[which(churnAnalysis[,"tenure"] >5 & churnAnalysis[,"tenure"] <70)] <- "between"
```
Apply the class transform to all 3 data sets
```{r}
churnAnalysis["MonthlyCharges"] <- MonthlyChargesClass
churnAnalysis["TotalCharges"] <- TotalChargesClass
churnAnalysis["tenure"] <- TenureClass
churnAnalysisTraining <- churnAnalysis[1:(nrow(churnAnalysis)/2),]
churnAnalysisValidation <- churnAnalysis[(nrow(churnAnalysis)/2+1): floor(3*nrow(churnAnalysis)/4),]
churnAnalysisTest <- churnAnalysis[floor(3*nrow(churnAnalysis)/4+1):nrow(churnAnalysis),]
```



```{r,echo=FALSE}
## convert the data from characters to factors so they can be used in decision tree algorithm
#test <- apply(churnAnalysisTraining,2,function(x) as.factor(x))
for(i in 1:ncol(churnAnalysisTraining)) {
  churnAnalysisTraining[,i] <- as.factor(churnAnalysisTraining[,i])
  churnAnalysisValidation[,i] <- as.factor(churnAnalysisValidation[,i])
  churnAnalysisTest[,i] <- as.factor(churnAnalysisTest[,i])
}
```
     
********    

    
********
    
### Use the rpart function from package: rpart

use rpart to run decision tree from package:rpart
```{r}
library(rpart)
## xval determine how many data points you want in the validation set.
fitTree <- rpart(Churn~gender+SeniorCitizen+Partner+Dependents+tenure+PhoneService+MultipleLines+InternetService+OnlineSecurity+OnlineBackup+DeviceProtection+TechSupport+StreamingTV+StreamingMovies+Contract+PaperlessBilling+PaymentMethod+MonthlyCharges+TotalCharges, churnAnalysisTraining,xval=0) 
pred_fitTree <- predict(fitTree, churnAnalysisTest,type="class")
table(Predicted = pred_fitTree, Actual = churnAnalysisTest$Churn) ## Confusion Matrix for Decision Tree
```

```{r}
plot(fitTree,uniform=TRUE)
text(fitTree,use.n=T,all=T)
printcp(fitTree) ## cp : complexity parameter
```
The cp at row one is the (rel error at row 2 - rel error at row 1)/nsplit in row 2.
The "rel error" is the relative error = absolute error/root node error



********

### Prune the tree using validation set
now prune the tree using the validation set

```{r}
TreeDepth <- nrow(fitTree$cptable)
for(i in 2:TreeDepth)


printcp(fitTree)
```

