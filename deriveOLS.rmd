---
title: "Derive OLS estimators"
---
The purpose of this section is to derive the estimators of OLS (Ordinary Least Square), their expectation and variance.
$$ y = x\beta + \epsilon$$

\[
\begin{bmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{T}
\end{bmatrix}=
\begin{bmatrix}
x_{1,1} & x_{1,2} & \dots & x_{1,K} \\
x_{2,1} & x_{2,2} & \dots & x_{2,K} \\
\vdots & \vdots & \ddots & \vdots \\
x_{T,1} & x_{T,2} & \dots & x_{T,K}
\end{bmatrix}
\begin{bmatrix}
\beta_{1}\\
\vdots\\
\beta_{K}
\end{bmatrix}+
\begin{bmatrix}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{T}
\end{bmatrix}
\]
note that 
$$ e = y-x\hat{\beta} $$ and

$$\epsilon  \neq e$$

**Goal: minimize sum of squared residuals** 
$$ min(e^{\prime} e) $$
$$
\begin{aligned}
e^{\prime} e &=  (y-x\hat{\beta})^{\prime}(y-x\hat{\beta}) \\
&= y^{\prime} y - \hat{\beta}^{\prime} x^{\prime} y - y^{\prime} x\hat{\beta} + \hat{\beta}^{\prime} x^{\prime} x \hat{\beta} \\
&= y^{\prime} y - 2\hat{\beta}^{\prime} x^{\prime} y + \hat{\beta}^{\prime} x^{\prime} x \hat{\beta} \\
\end{aligned}
$$
<div style="background-color:#E5E7E9;color:black;matrixOperation:20px;">
given that
$$ (AB)^{\prime} = B^{\prime} A^{\prime} $$
and transpose of a scaler is a scaler
$$ y^{\prime} x \hat{\beta} = (y^{\prime} x \hat{\beta})^{\prime} = (x\hat{\beta})^{\prime} y = \hat{\beta}^{\prime} x^{\prime} y $$
</div>


$$ \frac{\partial e^{\prime} e}{\partial \hat{\beta}} = -2x^{\prime} y+2x^{\prime} x \hat{\beta} = 0 $$


$$ \Rightarrow x^{\prime} x \hat{\beta} = x^{\prime} y $$
$$ 
\begin{aligned}
\Rightarrow \hat{\beta} &= (x^{\prime} x)^{-1}x^{\prime} y ,\,\ (\,if \,\, (x^{\prime} x)^{-1} \,\, exists) \\
 &= (x^{\prime} x)^{-1}x^{\prime} (x\beta + \epsilon) \\
 &= \beta+(x^{\prime} x)^{-1}x^{\prime}  \epsilon 
 \end{aligned}
 $$
<div style="background-color:white;color:blue;matrixOperation:20px;">
### Unbiasedness
</div>

$$ E[ \hat{\beta}] = \beta + E[(x^{\prime} x)^{-1}x^{\prime} \epsilon]$$

If $E[\epsilon] =0$ and $E[\epsilon | x] = 0$, then $E[(x^{\prime} x)^{-1}x^{\prime} \epsilon] = 0$ and $E[ \hat{\beta}] = \beta$.   
**$\Rightarrow \hat{\beta}$ is an unbiased estimator.**

********
<div style="background-color:white;color:blue;matrixOperation:20px;">
### Variance of the OLS estimator
</div>
recall the formula for variance
$$Var(A) = E[\,(A-E[A])\, (A-E[A])^{\prime}]$$
and the OLS estimator we just derived
$$\hat\beta_{OLS} = \beta + (x^{\prime} x)^{-1}x^{\prime}  \epsilon$$
and if the OLS estimator is an unbiased estimator
$$E[\hat\beta_{OLS}] = \beta$$
Then
$$
\begin{aligned}
Var(\hat\beta_{OLS}) &= E[(x^{\prime} x)^{-1}x^{\prime}\epsilon \,\, ((x^{\prime} x)^{-1}x^{\prime}\epsilon)^{\prime}] \\
&=(x^{\prime} x)^{-1}x^{\prime} \,\, E[\epsilon\epsilon^{\prime}] \,\, ((x^{\prime} x)^{-1}x^{\prime})^{\prime}
\end{aligned}
$$
if $E[\epsilon\epsilon^{\prime}]=\sigma^2I_T$ and $E[\epsilon|X]=0$ Then
$$
\begin{aligned}
Var(\hat\beta_{OLS}) &=(x^{\prime} x)^{-1}x^{\prime} \,\, E[\epsilon\epsilon^{\prime}] \,\, ((x^{\prime} x)^{-1}x^{\prime})^{\prime} \\
&=\sigma^2(X^{\prime} X)^{-1}
\end{aligned}
$$
We don't know the $\epsilon$...we only have the estimate of it-the residuals $e$ where $e = Y-X\hat\beta_{OLS}$.

Then we estimate the $\sigma^2$ by
$$s^2= \frac{e^{\prime} e}{T-K} $$ 
where $K$ is the number of explanatory variables

Therefore, our estimate of variance of $\hat\beta_{OLS}$ becomes
$$s^2(X^{\prime} X)^{-1}$$
Then the distribution of $\hat\beta_{OLS}$ becomes a t-distribution with $T-K$ degree of freedoms with $mean = \beta$ and $variance = s^2(X^{^{\prime}} X)^{-1}$




