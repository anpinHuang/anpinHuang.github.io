---
title: "Derive OLS estimators"
---
The purpose of this section is to derive the estimators of OL, their expectation and variance.
$$ y = x\beta + \epsilon$$

$$ e = y-x\hat{\beta} $$

note that $$\epsilon  \neq e$$

goal: minimize sum of squared residuals $$ min(e\prime e) $$ 
$$
\begin{aligned}
e\prime e &=  (y-x\hat{\beta})\prime(y-x\hat{\beta}) \\
&= y\prime y - \hat{\beta}\prime x\prime y - y\prime x\hat{\beta} + \hat{\beta}\prime x\prime x \hat{\beta} \\
&= y\prime y - 2\hat{\beta}\prime x\prime y + \hat{\beta}\prime x\prime x \hat{\beta} \\
\end{aligned}
$$

given that
*** $$ (AB)\prime = B\prime A\prime $$
and transpose of a scaler is a scaler
*** $$ y\prime x \hat{\beta} = (y\prime x \hat{\beta})\prime = (x\hat{\beta})\prime y = \hat{\beta}\prime x\prime y $$

$$ \frac{\partial e\prime e}{\partial \hat{\beta}} = -2x\prime y+2x\prime x \hat{\beta} = 0 $$


$$ \Rightarrow x\prime x \hat{\beta} = x\prime y $$
$$ \Rightarrow \hat{\beta} = (x\prime x)^{-1}x\prime y ,\,\, if \,\, (x\prime x)^{-1} \,\, exists $$
$$ \Rightarrow \hat{\beta} = (x\prime x)^{-1}x\prime (x\beta + \epsilon)  $$

$$ E[ \hat{\beta}] = \beta + E[(x\prime x)^{-1}x\prime \epsilon]$$
if $$ E[\epsilon] =0 $$ and $$E[\epsilon | x] = 0 $$, then $$ E[(x\prime x)^{-1}x\prime \epsilon] = 0 $$ and $$E[ \hat{\beta}] = \beta $$. $$\hat{\beta}$$ is an unbiased estimator.


